# -*- coding: utf-8 -*-
"""Copy of Jack's Machine Learning NBA Predictions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cRuNG_DBz0pogy_nb3_TwKzl0NEjMfB6

### DATA CLEANING PIPELINE

Import libraries, data, and clean datasets
"""

#@title

# Imports and set up

import requests
import csv
from google.colab import drive
import pandas as pd
from pandas import DataFrame, merge

# Adding google drive documents
drive.mount('/content/gdrive')


# Loading datasets
# Stats from NBA Games from 2015
#  https://drive.google.com/uc?export=do...
df_games = pd.read_csv('/content/gdrive/Shareddrives/ML_Final_Project/Stats/detail_nba_games.csv')
# Elo Stats from NBA games since 1947
df_elo = pd.read_csv('/content/gdrive/Shareddrives/ML_Final_Project/Stats/2014_nba_elo.csv')

# For viewing purposes
pd.set_option('display.max_columns', None)



# Clean up a little
df = df_games

df['date'] =pd.to_datetime(df.date)
df = df.rename(columns = {'team1' : 'home_team', 'team2':'away_team'})

del df['mp.1']
del df['mp_opp.1']
del df['index_opp']
del df['Unnamed: 0'] ## what is Unnamed: O

df = df.sort_values('date')
df = df.reset_index(drop=True)

"""Get result of next game. Then, clean up"""

# Setup

# Get the result from the next game

def next_game(team):
  team["next_game"] = team["won"].shift(-1)
  return team
df = df.groupby("team",group_keys=False).apply(next_game)


# denote the last game of the season with the number 2
df["next_game"][pd.isnull(df["next_game"])] = 2 
df["next_game"] = df["next_game"].astype(int, errors="ignore")


# Clean 

# Remove all columns with null values
nulls = pd.isnull(df)
nulls = nulls.sum()
nulls = nulls[nulls>0]
valid_cols = df.columns[~df.columns.isin(nulls.index)]
df = df[valid_cols].copy()


# ridge regression works best with scaled data
dont_scale = ["season","season_y","date","won","next_game",
              "team","team_opp","away_team","home_team"]
scale_cols = df.columns[~df.columns.isin(dont_scale)]

"""
Add rolling stats, if next game is home, and stats on next opponent"""

df_rolling = df[list(scale_cols) + ["won","team","season"]]

# add data that averages each teams' stats from the past 10 games 
# for a given season
def rolling_averages(team):
  rolling = team.rolling(10).mean()
  return rolling

df_rolling = df_rolling.groupby(["team", "season"], 
                                group_keys=False).apply(rolling_averages)


rolling_cols = [f"{col}_10_avg" for col in df_rolling.columns]
df_rolling.columns = rolling_cols

df2 = pd.concat([df, df_rolling], axis=1) #df2

# need to eliminate nans (drop first 10 games) 
df2 = df2.dropna() #df

# scale_cols2 = df.columns[~df2.columns.isin(dont_scale)]

df_rolling2 = df[list(scale_cols) + ["won","team","season"]]


# add data that averages each teams' stats from the past 10 games 
# for a given season
def rolling_var(team):
  rolling = team.rolling(10).var()
  return rolling

df_rolling2 = df_rolling2.groupby(["team", "season"], 
                                group_keys=False).apply(rolling_var)


rolling_cols2 = [f"{col}_10_var" for col in df_rolling2.columns]
df_rolling2.columns = rolling_cols2

df3 = pd.concat([df2, df_rolling2], axis=1) #df2

# need to eliminate nans (drop first 10 games) 
df3 = df3.dropna() #df
# df2 = df3
df2 = df3

# Give algorithm information on if the next game is home/away and who 
# the opponent is

def shift_col(team, col_name):
  next_col = team[col_name].shift(-1)
  return next_col

def add_col(df, col_name):
  return df.groupby("team", group_keys=False).apply(lambda x: shift_col(x, col_name))

df2["home_next"] = add_col(df2, "home") # is next game home/away
df2["team_opp_next"] = add_col(df2, "team_opp") # who is the next opponenent?
df2["date_next"] = add_col(df2, "date") # when is there next game

df2 = df2.copy()

# Merge rolling averages with home/next opponent data

full = df2.merge(
    df2[rolling_cols + ["team_opp_next","date_next","team"]],
                left_on=["team","date_next"], 
                right_on=["team_opp_next", "date_next"])

remove_cols = ['team_x','team_opp','team_opp_next_x','team_opp_next_y','team_y','season',
 'date','won','next_game','team','team_opp','date_next']

valid_cols = full.columns[~full.columns.isin(remove_cols)]

"""Now add elo stats"""

full['date_next'] =pd.to_datetime(full['date_next'])
df_elo['date'] =pd.to_datetime(df_elo['date'])

# Create a dataset that:
# Has team_x stats from past 10 games, has team_x NEXT opponent's stats from past 10 games,
# Has team x elo_pre stats for next game, has team_x NEXT opponent's pre_elo stats
# So this dataset has 2 dates in it: The date of the current game which correlates a team's
# past 10 rolling averages, and the date of their next game which correlates to the
# elo_pre stats


# have to seperate into home and away because the game stats dataset has 2 entries
# for each game, once where "team" is home and once where "team" is away
# in nba_elo, team1 is home and team2 is away

# Although elo is already a cummulitive stat, it might also be interesting
# to take the elo rolling average and observe things like is the team trending up
# or down (although we can already use their past 10 "won" for that I think)


df_home = full[full['home'] == 1] 

df_home = pd.merge(df_home, df_elo, left_on=  ['date_next','team_x','team_opp_next_x'],
                   right_on= ['date','team1','team2'], 
                   how = 'inner')


df_away = full[full['home'] == 0] 
df_away
df_away = pd.merge(df_away, df_elo, left_on=  ['date_next', 'team_x','team_opp_next_x'],
                   right_on= ['date', 'team2','team1'], 
                   how = 'inner')

all = pd.concat([df_home, df_away], axis=0)
del all['elo1_post']
del all['elo2_post']
del all['carm-elo1_post']
del all['carm-elo2_post'] ## what is Unnamed: O

invalid_cols = ['neutral', 'playoff', 'team1', 'team2', 'carm-elo1_pre', 'carm-elo2_pre', 
                'carm-elo_prob1', 'carm-elo_prob2','raptor1_pre', 'raptor2_pre','raptor_prob1', 'raptor_prob2', 'score1', 
                'score2', 'quality','importance', 'total_rating']

for col in invalid_cols:
  del all[col]

all2 = all


del all2['season_y']
del all2['date_y']

all2 = all2.rename(columns={"season_x": "season", 'date_x': 'date'})


remove_cols2 = list(all2.columns[all2.dtypes=="object"]) + remove_cols 

valid_cols2 = all2.columns[~all2.columns.isin(remove_cols2)] #df2

"""### Machine Learning: Ridge Classification"""

# Extract most important features to avoid overfitting

from sklearn.model_selection import TimeSeriesSplit
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import RidgeClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score


# Initialize and setup
rr = RidgeClassifier(alpha=1)
split = TimeSeriesSplit(n_splits=3)

sfs = SequentialFeatureSelector(rr, n_features_to_select=40, # once we add rolling and next opponenent stats, higher is better?
                                direction="forward", cv=split)
scaler = MinMaxScaler()

# ridge regression works best with scaled data

df[scale_cols] = scaler.fit_transform(df[scale_cols])

# Use past seasons to predict future seasons

def backtest(data, model, predictors, start=2, step=1):

  all_predictions=[] # list of dataframes, where each df is predictions for a single season
  seasons = sorted(data["season"].unique())

  for i in range(start, len(seasons), step):
    season = seasons[i]

    train = data[data["season"] < season] # all data that comes before current season
    test = data[data["season"] == season] # current season

    # fit model with the 30 predictors and who wins their next game
    model.fit(train[predictors], train["next_game"])
    print(train[predictors].shape, train["next_game"].shape)
    
    # generate predictions on test set
    preds = model.predict(test[predictors])
    # convert numpy array to series (easier to work with)
    preds = pd.Series(preds,index=test.index)
    # juxtapose actual outcome to predicted outcome
    combined = pd.concat([test["next_game"], preds], axis=1)
    combined.columns = ["actual", "prediction"]

    # append predictions from each season
    all_predictions.append(combined)
  
  # concat all dfs
  return pd.concat(all_predictions)

scale_cols = all2.columns[all2.columns.isin(['elo1_pre','elo2_pre'])]
all2[scale_cols] = scaler.fit_transform(all2[scale_cols])

sfs.fit(all2[valid_cols2], all2["next_game"]) #df2

predictors3 = list(valid_cols2[sfs.get_support()])
print(predictors3)

predictions3 = backtest(all2, rr, predictors3) #df2

predictors3 = list(valid_cols2[sfs.get_support()])
# elo1 and elo2 might not be working because they are assigned to the wrong team sometimes
predictors3a = predictors3 + ['elo1_pre','elo2_pre'] + ['elo_prob1', 'elo_prob2',]

predictions3 = backtest(all2, rr, predictors3) #df2
accuracy_score(predictions3["actual"], predictions3["prediction"])

"""### Neural Network"""

import numpy as np
import matplotlib.pylab as plt
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Activation
from keras.layers import Conv2D, MaxPooling2D
from keras.datasets import mnist

"""Add one-hot encoders for teams"""

hot_cols = ['team_x','team_opp_next_x']

def one_hot(df, cols):
  for each in cols:
    dummies = pd.get_dummies(df[each], prefix=each, drop_first=True)
    df = pd.concat([df, dummies], axis=1)
  return df

all3 = one_hot(all2, hot_cols)

"""Training Method 1: Randomly Shuffled Training Data"""

all4 = all3.sample(frac=1).reset_index(drop=True)

train = all4.iloc[:2500]
test = all4.iloc[2500:]


exclude_columns = ['next_game', 'team_x', 'team_opp', 'date', 'won', 'date_next', 'team_opp_next_x', 'team_opp_next_y', 'team_y']

x_train = train[train.columns.difference(exclude_columns)]#.to_numpy()
x_test = test[test.columns.difference(exclude_columns)]#.to_numpy()


y_train = train['next_game']#.to_numpy()
y_test = test['next_game']#.to_numpy()

model = Sequential()
model.add(Dense(60, input_dim=614, activation = 'relu')) # play around
# model.add(Dense(10, activation='relu'))
model.add(Dropout(0.05)) # play around # try to avoid overfitting
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
              loss='binary_crossentropy', 
              metrics=['accuracy'])

print(model.summary())
history = model.fit(x_train, y_train, epochs=150, batch_size=100, validation_data=[x_test, y_test])

plt.hist(model.predict(x_test),100)

# Finetune
model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005), # play around
              loss='binary_crossentropy', 
              metrics=['accuracy'])
history2 = model.fit(x_train, y_train, epochs=50, batch_size=100, validation_data=[x_test, y_test])

"""Training Method 2: Use past seasons to predict current season"""

# def backtest(data, model, predictors, start=2, step=1):
def backtestNN(data, start=2, step=1):

  all_predictions=[]
  all_predictions2=[] # list of dataframes, where each df is predictions for a single season
  seasons = sorted(data["season"].unique())

  for i in range(start, len(seasons), step):
    season = seasons[i]
    print(" ")
    print(season)

    # train = data[data["season"] < season] # all data that comes before current season
    train = data[data["season"].isin([season-1,season-2])] # all data that comes before current season
    test = data[data["season"] == season] # current season

    exclude_columns = ['next_game', 'team_x', 'team_opp', 'date', 'won', 'date_next', 'team_opp_next_x', 'team_opp_next_y', 'team_y']
    
    x_train = train[train.columns.difference(exclude_columns)]#.to_numpy()
    x_test = test[test.columns.difference(exclude_columns)]#.to_numpy()
    

    y_train = train['next_game']#.to_numpy()
    y_test = test['next_game']#.to_numpy()

    model = Sequential()

    if season == 2018:
      # continue
      # print("A")

      # Model
      model.add(Dense(60, input_dim=614, activation = 'relu')) # play around
      model.add(Dropout(0.05)) # play around # try to avoid overfitting
      model.add(Dense(1, activation='sigmoid'))

      model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
                    loss='binary_crossentropy', 
                    metrics=['accuracy'])
      
      print(model.summary())
      history = model.fit(x_train, y_train, epochs=350, batch_size=100, validation_data=[x_test, y_test])
      
      plt.hist(model.predict(x_test),100)

      # Finetune
      model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005), # play around
                    loss='binary_crossentropy', 
                    metrics=['accuracy'])
      history2 = model.fit(x_train, y_train, epochs=50, batch_size=100, validation_data=[x_test, y_test])
      
    

    elif season == 2019:
      # continue
      # print("B")


      # Model
      model.add(Dense(60, input_dim=614, activation = 'relu')) # play around
      model.add(Dropout(0.05)) # play around # try to avoid overfitting
      model.add(Dense(1, activation='sigmoid'))

      model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
                    loss='binary_crossentropy', 
                    metrics=['accuracy'])
      
      print(model.summary())
      history = model.fit(x_train, y_train, epochs=350, batch_size=100, validation_data=[x_test, y_test])
      
      plt.hist(model.predict(x_test),100)

      # Finetune
      model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005), # play around
                    loss='binary_crossentropy', 
                    metrics=['accuracy'])
      history2 = model.fit(x_train, y_train, epochs=50, batch_size=100, validation_data=[x_test, y_test])


    elif season == 2020:
      # continue
      # print("C")

      # Model
      model.add(Dense(60, input_dim=614, activation = 'relu')) # play around
      model.add(Dropout(0.1)) # play around # try to avoid overfitting
      # model.add(Dense(10, input_dim=614, activation = 'relu')) # play around
      model.add(Dense(1, activation='sigmoid'))

      model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
                    loss='binary_crossentropy', 
                    metrics=['accuracy'])
      
      print(model.summary())
      history = model.fit(x_train, y_train, epochs=200, batch_size=100, validation_data=[x_test, y_test])
      
      plt.hist(model.predict(x_test),100)

      # Finetune
      model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005), # play around
                    loss='binary_crossentropy', 
                    metrics=['accuracy'])
      history2 = model.fit(x_train, y_train, epochs=20, batch_size=100, validation_data=[x_test, y_test])

    elif season == 2021:
      # print("D")
      
      # Model
      model.add(Dense(60, input_dim=614, activation = 'relu')) # play around
      model.add(Dropout(0.5)) # play around # try to avoid overfitting
      # model.add(Dense(60, activation = 'relu'))
      # model.add(Dropout(0.1)) # play around # try to avoid overfitting
      model.add(Dense(1, activation='sigmoid'))

      model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
                    loss='binary_crossentropy', 
                    metrics=['accuracy'])
      
      print(model.summary())
      history = model.fit(x_train, y_train, epochs=100, batch_size=100, validation_data=[x_test, y_test])
      
      plt.hist(model.predict(x_test),100)

      # Finetune
      model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005), # play around
                    loss='binary_crossentropy', 
                    metrics=['accuracy'])
      history2 = model.fit(x_train, y_train, epochs=20, batch_size=100, validation_data=[x_test, y_test])
      # history2=5

    elif season == 2022:
      print("E")
      
      # Model
      model.add(Dense(60, input_dim=614, activation = 'relu')) # play around
      model.add(Dropout(0.1)) # play around # try to avoid overfitting
      model.add(Dense(1, activation='sigmoid'))

      model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),
                    loss='binary_crossentropy', 
                    metrics=['accuracy'])
      
      print(model.summary())
      history = model.fit(x_train, y_train, epochs=100, batch_size=100, validation_data=[x_test, y_test])
      
      plt.hist(model.predict(x_test),100)

      # Finetune
      model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005), # play around
                    loss='binary_crossentropy', 
                    metrics=['accuracy'])
      history2 = model.fit(x_train, y_train, epochs=20, batch_size=100, validation_data=[x_test, y_test])


    print("______ _______")
    

    


    all_predictions.append(history)
    all_predictions2.append(history)


  return [all_predictions,all_predictions2]



predictions,predictions2 = backtestNN(all3, start=2, step=1)

plt1 = plt.plot(predictions[0].history['accuracy'], label='accuracy')
plt1 = plt.plot(predictions[0].history['val_accuracy'])
plt1 = plt.legend()
plt1


plt2 = plt.plot(predictions2[1].history['accuracy'], label='accuracy')
plt2 = plt.plot(predictions2[1].history['val_accuracy'])
plt2 = plt.legend()
plt2


plt3 = plt.plot(predictions[2].history['accuracy'], label='accuracy')
plt3 = plt.plot(predictions[2].history['val_accuracy'])
plt3 = plt.legend()
plt3

plt4 = plt.plot(predictions[3].history['accuracy'], label='accuracy')
plt4 = plt.plot(predictions[3].history['val_accuracy'])
plt4 = plt.legend()
plt4

plt5 = plt.plot(predictions[4].history['accuracy'], label='accuracy')
plt5 = plt.plot(predictions[4].history['val_accuracy'])
plt5 = plt.legend()
plt5
